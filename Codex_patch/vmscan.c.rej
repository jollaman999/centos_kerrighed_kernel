--- mm/vmscan.c
+++ mm/vmscan.c
@@ -97,7 +103,11 @@
 	unsigned long (*isolate_pages)(unsigned long nr, struct list_head *dst,
 			unsigned long *scanned, int order, int mode,
 			struct zone *z, struct mem_cgroup *mem_cont,
+#ifdef CONFIG_KRG_MM
+			int active, int file, int kddm);
+#else
 			int active, int file);
+#endif
 };
 
 #define lru_to_page(_head) (list_entry((_head)->prev, struct page, lru))
@@ -540,6 +550,10 @@
 		 * We know how to handle that.
 		 */
 		lru = active + page_is_file_cache(page);
+#ifdef CONFIG_KRG_MM
+		BUG_ON(page_is_migratable(page) && page_is_file_cache(page));
+		lru += page_is_migratable(page);
+#endif
 		lru_cache_add_lru(page, lru);
 	} else {
 		/*
@@ -582,11 +596,36 @@
 	VM_BUG_ON(PageLRU(page));
 
 	lru = !!TestClearPageActive(page) + page_is_file_cache(page);
+#ifdef CONFIG_KRG_MM
+	BUG_ON(page_is_migratable(page) && page_is_file_cache(page));
+	lru += page_is_migratable(page);
+#endif
 	lru_cache_add_lru(page, lru);
 	put_page(page);
 }
 #endif /* CONFIG_UNEVICTABLE_LRU */
 
+#ifdef CONFIG_KRG_MM
+static int check_injection_flow(void)
+{
+	long i = 0, limit = RPC_MAX_PAGES;
+
+	if ((rpc_consumed_bytes() / PAGE_SIZE) < limit)
+		return 0;
+
+	if (current_is_kswapd())
+		limit = limit / 2;
+	else
+		limit = 4 * limit / 5;
+
+	while ((rpc_consumed_bytes() / PAGE_SIZE) > limit) {
+		schedule();
+		i++;
+	}
+
+	return 0;
+}
+#endif
 
 /*
  * shrink_page_list() returns the number of reclaimed pages
@@ -1011,13 +1087,21 @@
 					unsigned long *scanned, int order,
 					int mode, struct zone *z,
 					struct mem_cgroup *mem_cont,
+#ifdef CONFIG_KRG_MM
+					int active, int file, int kddm)
+#else
 					int active, int file)
+#endif
 {
 	int lru = LRU_BASE;
 	if (active)
 		lru += LRU_ACTIVE;
 	if (file)
 		lru += LRU_FILE;
+#ifdef CONFIG_KRG_MM
+	if (kddm)
+		lru += LRU_MIGR;
+#endif
 	return isolate_lru_pages(nr, &z->lru[lru].list, dst, scanned, order,
 								mode, !!file);
 }
@@ -1035,6 +1119,10 @@
 
 	list_for_each_entry(page, page_list, lru) {
 		lru = page_is_file_cache(page);
+#ifdef CONFIG_KRG_MM
+		BUG_ON(page_is_migratable(page) && page_is_file_cache(page));
+		lru += page_is_migratable(page);
+#endif
 		if (PageActive(page)) {
 			lru += LRU_ACTIVE;
 			ClearPageActive(page);
@@ -1097,7 +1185,11 @@
  */
 static unsigned long shrink_inactive_list(unsigned long max_scan,
 			struct zone *zone, struct scan_control *sc,
+#ifdef CONFIG_KRG_MM
+			int priority, int file, int kddm)
+#else
 			int priority, int file)
+#endif
 {
 	LIST_HEAD(page_list);
 	struct pagevec pvec;
@@ -1132,7 +1224,11 @@
 
 		nr_taken = sc->isolate_pages(sc->swap_cluster_max,
 			     &page_list, &nr_scan, sc->order, mode,
+#ifdef CONFIG_KRG_MM
+				zone, sc->mem_cgroup, 0, file, kddm);
+#else
 				zone, sc->mem_cgroup, 0, file);
+#endif
 		nr_active = clear_active_flags(&page_list, count);
 		__count_vm_events(PGDEACTIVATE, nr_active);
 
@@ -1221,7 +1327,11 @@
 			lru = page_lru(page);
 			add_page_to_lru_list(zone, page, lru);
 			if (PageActive(page)) {
+#ifdef CONFIG_KRG_MM
+				int file = reclaim_stat_index (page);
+#else
 				int file = !!page_is_file_cache(page);
+#endif
 				reclaim_stat->recent_rotated[file]++;
 			}
 			if (!pagevec_add(&pvec, page)) {
@@ -1272,7 +1382,11 @@
 
 
 static void shrink_active_list(unsigned long nr_pages, struct zone *zone,
+#ifdef CONFIG_KRG_MM
+		struct scan_control *sc, int priority, int file, int kddm)
+#else
 			struct scan_control *sc, int priority, int file)
+#endif
 {
 	unsigned long pgmoved;
 	int pgdeactivate = 0;
@@ -1288,7 +1402,11 @@
 	spin_lock_irq(&zone->lru_lock);
 	pgmoved = sc->isolate_pages(nr_pages, &l_hold, &pgscanned, sc->order,
 					ISOLATE_ACTIVE, zone,
+#ifdef CONFIG_KRG_MM
+					sc->mem_cgroup, 1, file, kddm);
+#else
 					sc->mem_cgroup, 1, file);
+#endif
 	/*
 	 * zone->pages_scanned is used for detect zone's oom
 	 * mem_cgroup remembers nr_scan by itself.
@@ -1296,14 +1414,25 @@
 	if (scanning_global_lru(sc)) {
 		zone->pages_scanned += pgscanned;
 	}
+#ifdef CONFIG_KRG_MM
+	reclaim_stat->recent_scanned[RECLAIM_STAT_INDEX(file, kddm)] += pgmoved;
+#else
 	reclaim_stat->recent_scanned[!!file] += pgmoved;
+#endif
 
 	if (file)
 		__mod_zone_page_state(zone, NR_ACTIVE_FILE, -pgmoved);
 	else
+#ifdef CONFIG_KRG_MM
+	if (kddm)
+		__mod_zone_page_state(zone, NR_ACTIVE_MIGR, -pgmoved);
+	else
+#endif
 		__mod_zone_page_state(zone, NR_ACTIVE_ANON, -pgmoved);
 	spin_unlock_irq(&zone->lru_lock);
 
+
+
 	pgmoved = 0;
 	while (!list_empty(&l_hold)) {
 		cond_resched();
@@ -1327,7 +1456,11 @@
 	 * Move the pages to the [file or anon] inactive list.
 	 */
 	pagevec_init(&pvec, 1);
+#ifdef CONFIG_KRG_MM
+	lru = BUILD_LRU_ID(0 /* inactive */, file, kddm);
+#else
 	lru = LRU_BASE + file * LRU_FILE;
+#endif
 
 	spin_lock_irq(&zone->lru_lock);
 	/*
@@ -1336,7 +1469,11 @@
 	 * This helps balance scan pressure between file and anonymous
 	 * pages in get_scan_ratio.
 	 */
+#ifdef CONFIG_KRG_MM
+	reclaim_stat->recent_rotated[RECLAIM_STAT_INDEX(file, kddm)] += pgmoved;
+#else
 	reclaim_stat->recent_rotated[!!file] += pgmoved;
+#endif
 
 	pgmoved = 0;
 	while (!list_empty(&l_inactive)) {
@@ -1403,21 +1540,65 @@
 	return low;
 }
 
+#ifdef CONFIG_KRG_MM
+static int inactive_kddm_is_low_global(struct zone *zone)
+{
+	unsigned long active, inactive;
+
+	active = zone_page_state(zone, NR_ACTIVE_MIGR);
+	inactive = zone_page_state(zone, NR_INACTIVE_MIGR);
+
+	if (inactive * zone->inactive_ratio < active)
+		return 1;
+
+	return 0;
+}
+
+static int inactive_kddm_is_low(struct zone *zone, struct scan_control *sc)
+{
+	int low;
+
+	if (scanning_global_lru(sc))
+		low = inactive_kddm_is_low_global(zone);
+	else
+		BUG();
+	return low;
+}
+#endif
+
 static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,
 	struct zone *zone, struct scan_control *sc, int priority)
 {
 	int file = is_file_lru(lru);
 
 	if (lru == LRU_ACTIVE_FILE) {
+#ifdef CONFIG_KRG_MM
+		shrink_active_list(nr_to_scan, zone, sc, priority, file, 0);
+#else
 		shrink_active_list(nr_to_scan, zone, sc, priority, file);
+#endif
 		return 0;
 	}
 
 	if (lru == LRU_ACTIVE_ANON && inactive_anon_is_low(zone, sc)) {
+#ifdef CONFIG_KRG_MM
+		shrink_active_list(nr_to_scan, zone, sc, priority, file, 0);
+#else
 		shrink_active_list(nr_to_scan, zone, sc, priority, file);
+#endif
 		return 0;
 	}
+#ifdef CONFIG_KRG_MM
+	if (lru == LRU_ACTIVE_MIGR && inactive_kddm_is_low(zone, sc)) {
+		shrink_active_list(nr_to_scan, zone, sc, priority, 0, 1);
+		return 0;
+	}
+
+	return shrink_inactive_list(nr_to_scan, zone, sc, priority, file,
+				    is_kddm_lru(lru));
+#else
 	return shrink_inactive_list(nr_to_scan, zone, sc, priority, file);
+#endif
 }
 
 /*
@@ -1432,23 +1613,31 @@
 static void get_scan_ratio(struct zone *zone, struct scan_control *sc,
 					unsigned long *percent)
 {
+#ifdef CONFIG_KRG_MM
+	unsigned long kddm, kddm_prio, kp;
+#endif
 	unsigned long anon, file, free;
 	unsigned long anon_prio, file_prio;
 	unsigned long ap, fp;
 	struct zone_reclaim_stat *reclaim_stat = get_reclaim_stat(zone, sc);
 
+#ifndef CONFIG_KRG_MM
 	/* If we have no swap space, do not bother scanning anon pages. */
 	if (!sc->may_swap || (nr_swap_pages <= 0)) {
 		percent[0] = 0;
 		percent[1] = 100;
 		return;
 	}
+#endif
 
 	anon  = zone_nr_pages(zone, sc, LRU_ACTIVE_ANON) +
 		zone_nr_pages(zone, sc, LRU_INACTIVE_ANON);
 	file  = zone_nr_pages(zone, sc, LRU_ACTIVE_FILE) +
 		zone_nr_pages(zone, sc, LRU_INACTIVE_FILE);
-
+#ifdef CONFIG_KRG_MM
+	kddm  = zone_nr_pages(zone, sc, LRU_ACTIVE_MIGR) +
+		zone_nr_pages(zone, sc, LRU_INACTIVE_MIGR);
+#else
 	if (scanning_global_lru(sc)) {
 		free  = zone_page_state(zone, NR_FREE_PAGES);
 		/* If we have very few page cache pages,
@@ -1618,7 +1850,13 @@
 	 * rebalance the anon lru active/inactive ratio.
 	 */
 	if (inactive_anon_is_low(zone, sc))
+#ifdef CONFIG_KRG_MM
+		shrink_active_list(SWAP_CLUSTER_MAX, zone, sc, priority, 0, 0);
+	if (inactive_kddm_is_low(zone, sc))
+		shrink_active_list(SWAP_CLUSTER_MAX, zone, sc, priority, 0, 1);
+#else
 		shrink_active_list(SWAP_CLUSTER_MAX, zone, sc, priority, 0);
+#endif
 
 	throttle_vm_writeout(sc->gfp_mask);
 }
@@ -1919,7 +2157,15 @@
 			 */
 			if (inactive_anon_is_low(zone, &sc))
 				shrink_active_list(SWAP_CLUSTER_MAX, zone,
+#ifndef CONFIG_KRG_MM
 							&sc, priority, 0);
+#else
+							&sc, priority, 0, 0);
+			/* Do the same on kddm lru pages */
+			if (inactive_kddm_is_low(zone, &sc))
+				shrink_active_list(SWAP_CLUSTER_MAX, zone,
+						   &sc, priority, 0, 1);
+#endif
 
 			if (!zone_watermark_ok(zone, order, zone->pages_high,
 					       0, 0)) {
@@ -2148,6 +2394,10 @@
 {
 	return global_page_state(NR_ACTIVE_ANON)
 		+ global_page_state(NR_ACTIVE_FILE)
+#ifdef CONFIG_KRG_MM
+		+ global_page_state(NR_ACTIVE_MIGR)
+		+ global_page_state(NR_INACTIVE_MIGR)
+#endif
 		+ global_page_state(NR_INACTIVE_ANON)
 		+ global_page_state(NR_INACTIVE_FILE);
 }
