--- arch/x86/lib/usercopy_32.c
+++ arch/x86/lib/usercopy_32.c
@@ -824,6 +863,11 @@
 unsigned long __copy_from_user_ll(void *to, const void __user *from,
 					unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n = krg_copy_user_generic(to, from, n, 1);
+	else
+#endif
 	if (movsl_is_ok(to, from, n))
 		__copy_user_zeroing(to, from, n);
 	else
@@ -835,6 +879,11 @@
 unsigned long __copy_from_user_ll_nozero(void *to, const void __user *from,
 					 unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n = krg_copy_user_generic(to, from, n, 0);
+	else
+#endif
 	if (movsl_is_ok(to, from, n))
 		__copy_user(to, from, n);
 	else
@@ -847,6 +896,11 @@
 unsigned long __copy_from_user_ll_nocache(void *to, const void __user *from,
 					unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n = krg_copy_user_generic(to, from, n, 1);
+	else
+#endif
 #ifdef CONFIG_X86_INTEL_USERCOPY
 	if (n > 64 && cpu_has_xmm2)
 		n = __copy_user_zeroing_intel_nocache(to, from, n);
@@ -862,6 +916,11 @@
 unsigned long __copy_from_user_ll_nocache_nozero(void *to, const void __user *from,
 					unsigned long n)
 {
+#ifdef CONFIG_KRG_FAF
+	if (check_ruaccess())
+		n = krg_copy_user_generic(to, from, n, 0);
+	else
+#endif
 #ifdef CONFIG_X86_INTEL_USERCOPY
 	if (n > 64 && cpu_has_xmm2)
 		n = __copy_user_intel_nocache(to, from, n);
