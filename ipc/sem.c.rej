--- ipc/sem.c
+++ ipc/sem.c
@@ -86,13 +86,29 @@
 
 #include <asm/uaccess.h>
 #include "util.h"
+#ifdef CONFIG_KRG_IPC
+#include <linux/random.h>
+#include <kerrighed/pid.h>
+#ifdef CONFIG_KRG_EPM
+#include <kerrighed/action.h>
+#endif
+#include "krgsem.h"
+#endif
+
+#ifdef CONFIG_KRG_IPC
+#define assert_mutex_locked(x) BUG_ON(!mutex_is_locked(x))
+#endif
 
 #define sem_ids(ns)	((ns)->ids[IPC_SEM_IDS])
 
+#ifndef CONFIG_KRG_IPC
 #define sem_unlock(sma)		ipc_unlock(&(sma)->sem_perm)
+#endif
 #define sem_checkid(sma, semid)	ipc_checkid(&sma->sem_perm, semid)
 
+#ifndef CONFIG_KRG_IPC
 static int newary(struct ipc_namespace *, struct ipc_params *);
+#endif
 static void freeary(struct ipc_namespace *, struct kern_ipc_perm *);
 #ifdef CONFIG_PROC_FS
 static int sysvipc_sem_proc_show(struct seq_file *s, void *it);
@@ -129,6 +145,7 @@
 void sem_exit_ns(struct ipc_namespace *ns)
 {
 	free_ipcs(ns, &sem_ids(ns), freeary);
+	idr_destroy(&ns->ids[IPC_SEM_IDS].ipcs_idr);
 }
 #endif
 
@@ -144,7 +161,11 @@
  * sem_lock_(check_) routines are called in the paths where the rw_mutex
  * is not held.
  */
+#ifdef CONFIG_KRG_IPC
+struct sem_array *sem_lock(struct ipc_namespace *ns, int id)
+#else
 static inline struct sem_array *sem_lock(struct ipc_namespace *ns, int id)
+#endif
 {
 	struct kern_ipc_perm *ipcp = ipc_lock(&sem_ids(ns), id);
 
@@ -154,7 +175,10 @@
 	return container_of(ipcp, struct sem_array, sem_perm);
 }
 
-static inline struct sem_array *sem_lock_check(struct ipc_namespace *ns,
+#ifndef CONFIG_KRG_IPC
+static inline
+#endif
+struct sem_array *sem_lock_check(struct ipc_namespace *ns,
 						int id)
 {
 	struct kern_ipc_perm *ipcp = ipc_lock_check(&sem_ids(ns), id);
@@ -285,6 +318,20 @@
 	INIT_LIST_HEAD(&sma->list_id);
 	sma->sem_nsems = nsems;
 	sma->sem_ctime = get_seconds();
+#ifdef CONFIG_KRG_IPC
+	INIT_LIST_HEAD(&sma->remote_sem_pending);
+
+	if (is_krg_ipc(&sem_ids(ns))) {
+		retval = krg_ipc_sem_newary(ns, sma);
+		if (retval) {
+			security_sem_free(sma);
+			ipc_rcu_putref(sma);
+			return retval;
+		}
+	} else
+
+	sma->sem_perm.krgops = NULL;
+#endif
 	sem_unlock(sma);
 
 	return sma->sem_perm.id;
@@ -414,8 +461,27 @@
 	int error;
 	struct sem_queue * q;
 
+#ifdef CONFIG_KRG_IPC
+	/* the following is used to ensure that a node would not
+	   keep the sem for it */
+	int remote = 0, loop = 0;
+	if (sma->sem_perm.krgops) {
+		remote = get_random_int()%2;
+		loop = 1;
+	}
+begin:
+	if (remote)
+		q = list_entry(sma->remote_sem_pending.next, struct sem_queue, list);
+	else
+#endif
+
 	q = list_entry(sma->sem_pending.next, struct sem_queue, list);
+#ifdef CONFIG_KRG_IPC
+	while ((!remote && &q->list != &sma->sem_pending)
+	       || (remote && &q->list != &sma->remote_sem_pending)) {
+#else
 	while (&q->list != &sma->sem_pending) {
+#endif
 		error = try_atomic_semop(sma, q->sops, q->nsops,
 					 q->undo, q->pid);
 
@@ -442,6 +508,12 @@
 			 */
 			if (q->alter) {
 				list_del(&q->list);
+#ifdef CONFIG_KRG_IPC
+				if (remote)
+					n = list_entry(sma->remote_sem_pending.next,
+						       struct sem_queue, list);
+				else
+#endif
 				n = list_entry(sma->sem_pending.next,
 						struct sem_queue, list);
 			} else {
@@ -453,6 +525,11 @@
 			/* wake up the waiting thread */
 			q->status = IN_WAKEUP;
 
+#ifdef CONFIG_KRG_IPC
+			if (remote)
+				krg_ipc_sem_wakeup_process(q, error);
+			else
+#endif
 			wake_up_process(q->sleeper);
 			/* hands-off: q will disappear immediately after
 			 * writing q->status.
@@ -498,6 +582,18 @@
 			    && !(sops[i].sem_flg & IPC_NOWAIT))
 				semncnt++;
 	}
+#ifdef CONFIG_KRG_IPC
+	list_for_each_entry(q, &sma->remote_sem_pending, list) {
+		struct sembuf * sops = q->sops;
+		int nsops = q->nsops;
+		int i;
+		for (i = 0; i < nsops; i++)
+			if (sops[i].sem_num == semnum
+			    && (sops[i].sem_op < 0)
+			    && !(sops[i].sem_flg & IPC_NOWAIT))
+				semncnt++;
+	}
+#endif
 	return semncnt;
 }
 
@@ -517,6 +613,18 @@
 			    && !(sops[i].sem_flg & IPC_NOWAIT))
 				semzcnt++;
 	}
+#ifdef CONFIG_KRG_IPC
+	list_for_each_entry(q, &sma->remote_sem_pending, list) {
+		struct sembuf * sops = q->sops;
+		int nsops = q->nsops;
+		int i;
+		for (i = 0; i < nsops; i++)
+			if (sops[i].sem_num == semnum
+			    && (sops[i].sem_op == 0)
+			    && !(sops[i].sem_flg & IPC_NOWAIT))
+				semzcnt++;
+	}
+#endif
 	return semzcnt;
 }
 
@@ -530,14 +638,35 @@
  * as a writer and the spinlock for this semaphore set hold. sem_ids.rw_mutex
  * remains locked on exit.
  */
+#ifdef CONFIG_KRG_IPC
 static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
 {
+	if (is_krg_ipc(&sem_ids(ns)))
+		krg_ipc_sem_freeary(ns, ipcp);
+	else
+		local_freeary(ns, ipcp);
+}
+
+void local_freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
+#else
+static void freeary(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp)
+#endif
+{
 	struct sem_undo *un, *tu;
 	struct sem_queue *q, *tq;
 	struct sem_array *sma = container_of(ipcp, struct sem_array, sem_perm);
 
+#ifdef CONFIG_KRG_IPC
+	if (is_krg_ipc(&sem_ids(ns)))
+		BUG_ON(!list_empty(&sma->list_id));
+#endif
+
 	/* Free the existing undo structures for this semaphore set.  */
+#ifdef CONFIG_KRG_IPC
+	assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 	assert_spin_locked(&sma->sem_perm.lock);
+#endif
 	list_for_each_entry_safe(un, tu, &sma->list_id, list_id) {
 		list_del(&un->list_id);
 		spin_lock(&un->ulp->lock);
@@ -556,10 +685,26 @@
 		smp_wmb();
 		q->status = -EIDRM;	/* hands-off q */
 	}
+#ifdef CONFIG_KRG_IPC
+	list_for_each_entry_safe(q, tq, &sma->remote_sem_pending, list) {
+		list_del(&q->list);
+
+		/* __freeary is called on every nodes where the semarray exists:
+		 * no need to care about remote pending processes */
+		if (q->undo)
+			kfree(q->undo);
+
+		free_semqueue(q);
+	}
+#endif
 
 	/* Remove the semaphore set from the IDR */
 	sem_rmid(ns, sma);
+#ifdef CONFIG_KRG_IPC
+	local_sem_unlock(sma);
+#else
 	sem_unlock(sma);
+#endif
 
 	ns->used_sems -= sma->sem_nsems;
 	security_sem_free(sma);
@@ -634,15 +779,32 @@
 		struct semid64_ds tbuf;
 		int id;
 
+#ifdef CONFIG_KRG_IPC
+		down_read(&sem_ids(ns).rw_mutex);
+#endif
 		if (cmd == SEM_STAT) {
 			sma = sem_lock(ns, semid);
 			if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+			{
+				up_read(&sem_ids(ns).rw_mutex);
 				return PTR_ERR(sma);
+			}
+#else
+				return PTR_ERR(sma);
+#endif
 			id = sma->sem_perm.id;
 		} else {
 			sma = sem_lock_check(ns, semid);
 			if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+			{
+				up_read(&sem_ids(ns).rw_mutex);
+				return PTR_ERR(sma);
+			}
+#else
 				return PTR_ERR(sma);
+#endif
 			id = 0;
 		}
 
@@ -661,6 +823,9 @@
 		tbuf.sem_ctime  = sma->sem_ctime;
 		tbuf.sem_nsems  = sma->sem_nsems;
 		sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+		up_read(&sem_ids(ns).rw_mutex);
+#endif
 		if (copy_semid_to_user (arg.buf, &tbuf, version))
 			return -EFAULT;
 		return id;
@@ -671,6 +836,9 @@
 	return err;
 out_unlock:
 	sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 	return err;
 }
 
@@ -684,9 +852,19 @@
 	ushort* sem_io = fast_sem_io;
 	int nsems;
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&sem_ids(ns).rw_mutex);
+#endif
 	sma = sem_lock_check(ns, semid);
 	if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+	{
+		up_read(&sem_ids(ns).rw_mutex);
+		return PTR_ERR(sma);
+	}
+#else
 		return PTR_ERR(sma);
+#endif
 
 	nsems = sma->sem_nsems;
 
@@ -706,10 +884,18 @@
 		int i;
 
 		if(nsems > SEMMSL_FAST) {
+#ifndef CONFIG_KRG_IPC
 			sem_getref_and_unlock(sma);
+#endif
 
 			sem_io = ipc_alloc(sizeof(ushort)*nsems);
 			if(sem_io == NULL) {
+#ifdef CONFIG_KRG_IPC
+				err = -ENOMEM;
+				goto out_unlock;
+			}
+			BUG_ON(sma->sem_perm.deleted);
+#else
 				sem_putref(sma);
 				return -ENOMEM;
 			}
@@ -735,41 +922,65 @@
 	{
 		int i;
 		struct sem_undo *un;
-
+#ifndef CONFIG_KRG_IPC
 		sem_getref_and_unlock(sma);
+#endif
 
 		if(nsems > SEMMSL_FAST) {
 			sem_io = ipc_alloc(sizeof(ushort)*nsems);
 			if(sem_io == NULL) {
+#ifdef CONFIG_KRG_IPC
+				err = -ENOMEM;
+				goto out_unlock;
+#else
 				sem_putref(sma);
 				return -ENOMEM;
+#endif
 			}
 		}
 
 		if (copy_from_user (sem_io, arg.array, nsems*sizeof(ushort))) {
+#ifdef CONFIG_KRG_IPC
+			err = -EFAULT;
+			goto out_unlock;
+#else
 			sem_putref(sma);
 			err = -EFAULT;
 			goto out_free;
+#endif
 		}
 
 		for (i = 0; i < nsems; i++) {
 			if (sem_io[i] > SEMVMX) {
+#ifdef CONFIG_KRG_IPC
+				err = -ERANGE;
+				goto out_unlock;
+#else
 				sem_putref(sma);
 				err = -ERANGE;
 				goto out_free;
+#endif
 			}
 		}
+#ifdef CONFIG_KRG_IPC
+		BUG_ON(sma->sem_perm.deleted);
+#else
 		sem_lock_and_putref(sma);
 		if (sma->sem_perm.deleted) {
 			sem_unlock(sma);
 			err = -EIDRM;
 			goto out_free;
 		}
+#endif
 
 		for (i = 0; i < nsems; i++)
 			sma->sem_base[i].semval = sem_io[i];
 
+#ifdef CONFIG_KRG_IPC
+		assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 		assert_spin_locked(&sma->sem_perm.lock);
+#endif
 		list_for_each_entry(un, &sma->list_id, list_id) {
 			for (i = 0; i < nsems; i++)
 				un->semadj[i] = 0;
@@ -810,7 +1021,12 @@
 		if (val > SEMVMX || val < 0)
 			goto out_unlock;
 
+
+#ifdef CONFIG_KRG_IPC
+		assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 		assert_spin_locked(&sma->sem_perm.lock);
+#endif
 		list_for_each_entry(un, &sma->list_id, list_id)
 			un->semadj[semnum] = 0;
 
@@ -1025,20 +1244,41 @@
 
 	/* no undo structure around - allocate one. */
 	/* step 1: figure out the size of the semaphore array */
+#ifdef CONFIG_KRG_IPC
+	down_read(&sem_ids(ns).rw_mutex);
+#endif
 	sma = sem_lock_check(ns, semid);
 	if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+	{
+		up_read(&sem_ids(ns).rw_mutex);
+		return ERR_PTR(PTR_ERR(sma));
+	}
+#else
 		return ERR_PTR(PTR_ERR(sma));
+#endif
 
 	nsems = sma->sem_nsems;
+
+#ifndef CONFIG_KRG_IPC
 	sem_getref_and_unlock(sma);
+#endif
 
 	/* step 2: allocate new undo structure */
 	new = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);
 	if (!new) {
+#ifdef CONFIG_KRG_IPC
+		sem_unlock(sma);
+		up_read(&sem_ids(ns).rw_mutex);
+#else
 		sem_putref(sma);
+#endif
 		return ERR_PTR(-ENOMEM);
 	}
 
+#ifdef CONFIG_KRG_IPC
+	BUG_ON(sma->sem_perm.deleted);
+#else
 	/* step 3: Acquire the lock on semaphore array */
 	sem_lock_and_putref(sma);
 	if (sma->sem_perm.deleted) {
@@ -1076,6 +1321,9 @@
 	spin_unlock(&ulp->lock);
 	rcu_read_lock();
 	sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 out:
 	return un;
 }
@@ -1145,11 +1398,17 @@
 	} else
 		un = NULL;
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&sem_ids(ns).rw_mutex);
+#endif
 	sma = sem_lock_check(ns, semid);
 	if (IS_ERR(sma)) {
 		if (un)
 			rcu_read_unlock();
 		error = PTR_ERR(sma);
+#ifdef CONFIG_KRG_IPC
+		up_read(&sem_ids(ns).rw_mutex);
+#endif
 		goto out_free;
 	}
 
@@ -1216,6 +1485,10 @@
 	queue.undo = un;
 	queue.pid = task_tgid_vnr(current);
 	queue.alter = alter;
+#ifdef CONFIG_KRG_IPC
+	queue.semid = sma->sem_perm.id;
+	queue.node = kerrighed_node_id;
+#endif
 	if (alter)
 		list_add_tail(&queue.list, &sma->sem_pending);
 	else
@@ -1225,6 +1498,9 @@
 	queue.sleeper = current;
 	current->state = TASK_INTERRUPTIBLE;
 	sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 
 	if (timeout)
 		jiffies_left = schedule_timeout(jiffies_left);
@@ -1243,12 +1519,30 @@
 		goto out_free;
 	}
 
+#ifdef CONFIG_KRG_IPC
+	down_read(&sem_ids(ns).rw_mutex);
+#endif
 	sma = sem_lock(ns, semid);
 	if (IS_ERR(sma)) {
 		error = -EIDRM;
+#ifdef CONFIG_KRG_IPC
+		up_read(&sem_ids(ns).rw_mutex);
+#endif
 		goto out_free;
 	}
 
+#if defined(CONFIG_KRG_IPC) && defined(CONFIG_KRG_EPM)
+	if (krg_action_any_pending(current)) {
+#ifdef CONFIG_KRG_DEBUG
+		printk("%s:%d - action kerrighed! --> need replay!!\n",
+		       __PRETTY_FUNCTION__, __LINE__);
+#endif
+		list_del(&queue.list);
+		error = -ERESTARTSYS;
+		goto out_unlock_free;
+	}
+#endif
+
 	/*
 	 * If queue.status != -EINTR we are woken up by another process
 	 */
@@ -1266,6 +1560,9 @@
 
 out_unlock_free:
 	sem_unlock(sma);
+#ifdef CONFIG_KRG_IPC
+	up_read(&sem_ids(ns).rw_mutex);
+#endif
 out_free:
 	if(sops != fast_sops)
 		kfree(sops);
@@ -1390,7 +1750,6 @@
 		struct sem_array *sma;
 		struct sem_undo *un;
 		int semid;
-		int i;
 
 		rcu_read_lock();
 		un = list_entry(rcu_dereference(ulp->list_proc.next),
@@ -1404,64 +1763,83 @@
 		if (semid == -1)
 			break;
 
+#ifdef CONFIG_KRG_IPC
+		down_read(&sem_ids(tsk->nsproxy->ipc_ns).rw_mutex);
+#endif
 		sma = sem_lock_check(tsk->nsproxy->ipc_ns, un->semid);
 
 		/* exit_sem raced with IPC_RMID, nothing to do */
 		if (IS_ERR(sma))
+#ifdef CONFIG_KRG_IPC
+		{
+			up_read(&sem_ids(tsk->nsproxy->ipc_ns).rw_mutex);
 			continue;
+		}
+#else
+			continue;
+#endif
 
 		un = lookup_undo(ulp, semid);
 		if (un == NULL) {
 			/* exit_sem raced with IPC_RMID+semget() that created
 			 * exactly the same semid. Nothing to do.
 			 */
+#ifdef CONFIG_KRG_IPC
+			up_read(&sem_ids(tsk->nsproxy->ipc_ns).rw_mutex);
+#endif
 			sem_unlock(sma);
 			continue;
 		}
 
 		/* remove un from the linked lists */
+#ifdef CONFIG_KRG_IPC
+		assert_mutex_locked(&sma->sem_perm.mutex);
+#else
 		assert_spin_locked(&sma->sem_perm.lock);
+#endif
+
 		list_del(&un->list_id);
 
 		spin_lock(&ulp->lock);
 		list_del_rcu(&un->list_proc);
 		spin_unlock(&ulp->lock);
 
-		/* perform adjustments registered in un */
-		for (i = 0; i < sma->sem_nsems; i++) {
-			struct sem * semaphore = &sma->sem_base[i];
-			if (un->semadj[i]) {
-				semaphore->semval += un->semadj[i];
-				/*
-				 * Range checks of the new semaphore value,
-				 * not defined by sus:
-				 * - Some unices ignore the undo entirely
-				 *   (e.g. HP UX 11i 11.22, Tru64 V5.1)
-				 * - some cap the value (e.g. FreeBSD caps
-				 *   at 0, but doesn't enforce SEMVMX)
-				 *
-				 * Linux caps the semaphore value, both at 0
-				 * and at SEMVMX.
-				 *
-				 * 	Manfred <manfred@colorfullife.com>
-				 */
-				if (semaphore->semval < 0)
-					semaphore->semval = 0;
-				if (semaphore->semval > SEMVMX)
-					semaphore->semval = SEMVMX;
-				semaphore->sempid = task_tgid_vnr(current);
-			}
-		}
-		sma->sem_otime = get_seconds();
-		/* maybe some queued-up processes were waiting for this */
-		update_queue(sma);
+		__exit_sem_found(sma, un);
 		sem_unlock(sma);
-
+#ifdef CONFIG_KRG_IPC
+		up_read(&sem_ids(tsk->nsproxy->ipc_ns).rw_mutex);
+#endif
 		call_rcu(&un->rcu, free_un);
 	}
 	kfree(ulp);
 }
 
+#ifdef CONFIG_KRG_IPC
+void exit_sem(struct task_struct *tsk)
+{
+	struct ipc_namespace *ipcns;
+	struct nsproxy *ns;
+
+	ns = task_nsproxy(tsk);
+	if (!ns) { /* it happens when cleaning a failing fork */
+		if (krg_current)
+			ns = task_nsproxy(krg_current);
+		else
+			ns = task_nsproxy(current);
+	}
+
+	ipcns = ns->ipc_ns;
+
+	if (is_krg_ipc(&sem_ids(ipcns)))
+		krg_ipc_sem_exit_sem(ipcns, tsk);
+
+	/* let call __exit_sem in case process has been created
+	 * before the Kerrighed loading
+	 */
+	__exit_sem(tsk);
+}
+#endif
+
 #ifdef CONFIG_PROC_FS
 static int sysvipc_sem_proc_show(struct seq_file *s, void *it)
 {
